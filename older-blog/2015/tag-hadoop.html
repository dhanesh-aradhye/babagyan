<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="">
	<link rel="author" href="https://plus.google.com/103656420801452265269"/>
	<link rel="publisher" href="https://plus.google.com/113497864022889042631"/>

	<title>Hadoop - all posts | BabaGyan.com | Blog</title>
    <meta name="description" content="Usage of Hadoop"/>

	<meta name="twitter:card" content="summary"/>
	<meta name="twitter:site" content="@baba_gyan"/>
	<meta name="twitter:domain" content="BabaGyan"/>
	<meta name="twitter:creator" content="@baba_gyan"/>

    <!-- Bootstrap core CSS -->
    <link href="../css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="../css/babagyan.css" rel="stylesheet"> <style> 	.bg-secondary {     background-color: #f2d6f3;	} </style>


    <!-- Just for debugging purposes. Don't actually copy this line! -->
    <!--[if lt IE 9]><script src="../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->


<script type="application/javascript">  <script type="text/javascript">  if (window.top !== window.self) { window.top.location = window.self.location; } </script> <script type="application/javascript"> var doNotTrack = false; if (!doNotTrack) { window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;	 ga('create', 'UA-155045464-2', 'auto'); ga('send', 'pageview');} </script> <script async src='https://www.google-analytics.com/analytics.js'></script> </head>

  <body>

    <!-- Fixed navbar -->
    <div class="navbar navbar-default navbar-fixed-top" role="navigation"	>
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="/"><< - Back to NEW Home</a>
        </div>
        
      </div>
    </div>


    <div class="container">

<!-- Container 1 start -->
<div class="container col-md-8">

<!-- Jumbotron 1 start -->

<div class="jumbotron" id="j1">

<!-- Collapse group start -->
<div class="row">
<div class="collapse-group">

		<h3>
		<a title="BabaGyan.com | Blog Post" href="hadoop-architecture-birdview.html">Hadoop architecture birdview</a>
	</h3>
	<hr id="j1-hr">
<a title="Hadoop" href="http://hadoop.apache.org/?" target="_blank">Hadoop</a>'s architecture birdview as said by Mike Olson - CEO of Cloudera
<br/><br/>
Hadoop is designed to run on a large number of machines that don't share any memory or disks.
<br/><br/>
That means you can buy a whole bunch of commodity servers, slap them in a rack, and run the Hadoop software on each one.
<br/><br/>
<!-- Collapse text from here -->
<div class="collapse">

When you want to load all of your organization's data into Hadoop, what the software does is bust that data into pieces that it then spreads across your different servers. There's no one place where you go to talk to all of your data; Hadoop keeps track of where the data resides.
<br/>

And because there are multiple copy stores, data stored on a server that goes offline or dies can be automatically replicated from a known good copy.
<br/><br/>
<img class="alignleft size-full wp-image-666" alt="Hadoop" src="images.jpg" width="223" height="226" />
<br/><br/>
In a centralized database system, you have got one big disk connected to four or eight or 16 big processors. But that is as much horsepower as you can bring to bear. In a Hadoop cluster, every one of those servers has two or four or eight CPU cores.
<br/><br/>
You can run your indexing job by sending your code to each of the dozens of servers in your cluster, and each server operates on its own little piece of the data. Results are then delivered back to you in a unified whole. That's MapReduce: you map the operation out to all of those servers and then you reduce the results back into a single result set.
<br/><br/>
Architecturally, the reason you're able to deal with lots of data is because Hadoop spreads it out. And the reason you're able to ask complicated computational questions is because you have got all of these processors, working in parallel, harnessed together.
<br/><br/>

<!-- Collapse button here -->
</div>
<p><a class="btn btn-success btn-lg" href="#">Click to read more &raquo;</a></p>
</div> </div>
<!-- Collapse group end -->

<!-- Collapse group start -->
<div class="row">
<div class="collapse-group">

	<h3>
	<a title="BabaGyan.com | Blog Post" href="usage-hadoop.html">Usage of Hadoop</a>
	</h3>
	<hr id="j1-hr">
Usage of <a title="Hadoop" href="http://hadoop.apache.org/?" target="_blank">Hadoop</a>'s framework in industry - applicable to any of the industry.
<br/><br/>
Hadoop was originally conceived to solve the problem of storing large quantities of data at a very low cost even for the biggies like Google, Facebook or Yahoo. Low cost implementation of something which new and planned to be used often is very important.
<br/><br/>
Analytic applications are main output of Ha<span style="line-height: 1.5;">doop while the application categories can be broadly clubbed into three to four buckets.</span>
<br/><br/>
<strong>
1. Refine the data
</strong>
<br/><br/>
Irrespective of the verticle/domain, the Hadoop is being used to refine large quantities of data into informational data which is more manageable. The resulting data is loaded into the existing systems and can be simply accessed by traditional tools. Note that this new set of data is much richer than the earlier existing data set.
<br/><br/>
<strong>
2. Analysis of Data
</strong>
<!-- Collapse text from here -->
<div class="collapse">

<br/><br/>

Data Analysis' use case can often be where enterprises start by capturing data that was previously being discarded (exhaust data such as web logs, social media data, etc.). This data can be clubbed with other data and can be used to build more applications that uses the trends found in this data to make decisions.
<br/><br/>
<strong>
3. Enhancement of applications
</strong>
<br/><br/>
Existing applications (web or desktop or mobile) can be further enhanced by use of the data which Hadoop can provide. This can be used to provide user with better service which is customized so that the user comes to them instead of the competitor. Simply understanding of user patterns can achieve this for companies.
<br/><br/>
Also read - <a title="HADOOP ARCHITECTURE BIRDVIEW" href="hadoop-architecture-birdview.html" target="_blank">Hadoop architecture</a>
<br/>
<a href="images.jpg"><img class="alignleft size-full wp-image-666" alt="Hadoop" src="images.jpg" width="223" height="226" /></a>
<br/><br/>
<!-- Collapse button here -->
</div>
<p><a class="btn btn-success btn-lg" href="#">Click to read more &raquo;</a></p>
</div> </div>
<!-- Collapse group end -->

<!-- Collapse group start -->
<div class="row">
<div class="collapse-group">

	<h3>
	<a title="BabaGyan.com | Blog Post" href="hadoop-ubuntu-step-1.html">Hadoop and Ubuntu - step 1</a>
	</h3>
	<hr id="j1-hr">
<a title="Apache Hadoop" href="http://hadoop.apache.org/" target="_blank">Hadoop</a> and <a title="Ubuntu" href="http://www.ubuntu.com/" target="_blank">Ubuntu</a> - step 1 for setting up use of Hadoop using Ubuntu OS
<br/><br/>
There are basically two methods of using Hadoop:
<br/>
1. Configure Hadoop on Windows - this involves use of Hadoop setup, Cygwin tool, Java and Eclipse.
<br/>
I have configured this and initially configure it on my laptop, however, when I tried to perform the same configuration on another machine (to be used as another Hadoop node), Cygwin broke down.
<br/>
As a result, I was not able to complete the whole set up of Hadoop.
<br/><br/>
2. Configure Hadoop on Linux - Because of the above experience, I decided to go with the Linux based OS for Hadoop.
<br/><br/>

Using a Linux based OS is best approach for below reasons:
<br/>
<!-- Collapse text from here -->
<div class="collapse">
1. Hadoop is designed for Linux based systems (yes, it is)
<br/>
2. Hadoop requires SSH which is simple to configure in Linux (requires Cygwin in windows - Cygwin basically gives a feel and experience of Linux on Windows)
<br/>
3. It is a system which is naturally more secure on secure OSs, exibit A - Linux.
<br/><br/>


<strong>STEP 1 - Choose and configure (Linux) OS of choice on Machine of Choice</strong>
<br/>
<a href="00_01.png"><img class="alignleft size-medium wp-image-702" alt="00" src="00_01.png" width="300" height="202" /></a>
<br/><br/>
I chose Ubuntu - freely and easily available, good GUI based support for heavy Windows user
<br/><br/>
I chose to perform the installation on <a title="Oracle Virtual Box" href="https://www.virtualbox.org/" target="_blank">Virtual Box</a> - Open Source Virtualization tool by Oracle.
<br/><br/>
Down and install Virtual Box on your machine.
<br/><br/>
Download and install Ubuntu in the Virtual Box as a Guest OS
<br/><br/>
Get ready for further set up for Hadoop on Ubuntu
<br/><br/>
This completes step 1.
<br/><br/>
You can also learn about <a title="uses of Hadoop" href="usage-hadoop.html" target="_blank">usage of Hadoop</a> and about <a title="Hadoop architecture" href="hadoop-architecture-birdview.html" target="_blank">Hadoop architecture</a> on <a title="BabaGyan.com" href="http://babagyan.com/" target="_blank">BabaGyan.com</a>.
<br/><br/>
<!-- Collapse button here -->
</div>
<p><a class="btn btn-success btn-lg" href="#">Click to read more &raquo;</a></p>
</div> </div>
<!-- Collapse group end -->

<!-- Collapse group start -->
<div class="row">
<div class="collapse-group">

	<h3>
	<a title="BabaGyan.com | Blog Post" href="hadoop-ubuntu-step-2-install-oracle-java.html">Hadoop and Ubuntu - step 2</a>
	</h3>
	<hr id="j1-hr">
<a title="Apache Hadoop" href="http://hadoop.apache.org/" target="_blank">Hadoop</a> and <a title="Ubuntu" href="http://www.ubuntu.com/" target="_blank">Ubuntu</a> - step 2 - Install Oracle Java for Hadoop setup.
<br/><br/>
In the step 1 of the set up available <a title="Step 1" href="hadoop-ubuntu-step-1.html" target="_blank">here</a>,  we took a look at installation of Linux based OS (Ubuntu) for Hadoop as we opted for Linux instead of Windows for Hadoop. We also saw the reasons for the preference.
<br/><br/>
<span style="line-height: 1.5;"><a title="Step 1" href="hadoop-ubuntu-step-1.html" target="_blank">STEP 1</a>- Choose and configure (Linux) OS of choice on Machine of Choice</span>
<br/><br/>
<span style="line-height: 1.5;">STEP 2 - Install Java and configure it on the machine</span>
<br/><br/>
<span style="line-height: 1.5;">Available choices for Java are OpenJDK or Oracle JAVA. I preferred Oracle Java. Follow the below instructions for Oracle Java configuration on Ubuntu.</span>
<br/><br/>
Download the Oracle Java from its own official <a title="Oracle Java download page" href="http://www.oracle.com/technetwork/java/javase/downloads/index.html" target="_blank">download page</a>. Version should be compatible with your OS and machine type (32 or 64 bit). It will be now in some folder as Downloads.
<br/><br/>
Uncompress it from Terminal window using
<br/>
<!-- Collapse text from here -->
<div class="collapse">

<a href="00_01.png"><img class="alignleft size-medium wp-image-702" alt="00" src="00_01.png" width="300" height="202" /></a>
<br/><br/>
<pre>tar -xvf jdk-7u2-linux-x64.tar.gz</pre>
<br/>
The uncompressed directory (name depends on downloaded version, here jdk1.7.0) should be in /usr/lib under jvm. So lets move it there using. Only JRE will be available as Software through official Ubuntu. We need latest JRE with latest JDK from Oracle, hence we download.
<br/>
<pre>sudo mkdir -p /usr/lib/jvm
<br/>
sudo mv ./jdk1.7.0_02 /usr/lib/jvm/jdk1.7.0</pre>
<br/>
Set environment variables for Java. Open and append file <b>/etc/profile </b>with below code
<br/>
<pre><strong>JAVA_HOME=/usr/local/java/jdk1.7.0_45</strong>
<br/>
<strong>PATH=$PATH:$HOME/bin:$JAVA_HOME/bin</strong>
<br/>
<strong>JRE_HOME=/usr/local/java/jdk1.7.0/jre</strong>
<br/>
<strong>PATH=$PATH:$HOME/bin:$JRE_HOME/bin</strong>
<br/>
<strong>export JAVA_HOME</strong>
<br/>
<strong>export JRE_HOME</strong>
<br/>
<strong>export PATH</strong></pre>
<br/><br/>
Reboot now. After this, Ubuntu has to know that JDK is available; so run below commands
<br/>
<pre>sudo update-alternatives --install "/usr/bin/java" "java" "/usr/lib/jvm/jdk1.7.0/bin/java" 1</pre>
<br/><br/>
<pre>sudo update-alternatives --install "/usr/bin/javac" "javac" "/usr/lib/jvm/jdk1.7.0/bin/javac" 1</pre>
<br/><br/>
<pre>sudo update-alternatives --config java</pre>
<br/><br/>
Do the same for Javac. That's it, done. You can now check for Java and Javac version as
<br/><br/>
<code>java -version</code>
<br/><br/>
<pre><code>java version "1.7.0"
<br/>
Java(TM) SE Runtime Environment (build 1.7.0-b147)
<br/>
Java HotSpot(TM) Client VM (build 21.0-b17, mixed mode) </code></pre>
<br/><br/>
This completes step 2. You can also learn about <a title="uses of Hadoop" href="usage-hadoop.html" target="_blank">usage of Hadoop</a> and about <a title="Hadoop architecture" href="hadoop-architecture-birdview.html" target="_blank">Hadoop architecture </a>on <a title="BabaGyan.com" href="http://babagyan.com/" target="_blank">BabaGyan.com</a>.

<br/><br/>

<!-- Collapse button here -->
</div>
<p><a class="btn btn-success btn-lg" href="#">Click to read more &raquo;</a></p>
</div> </div>
<!-- Collapse group end -->

<!-- Collapse group start -->
<div class="row">
<div class="collapse-group">

	<h3>
	<a title="BabaGyan.com | Blog Post" href="hadoop-ubuntu-step-3-set-up-ssh.html">Hadoop and Ubuntu - step 3</a>
	</h3>
	<hr id="j1-hr">
<a title="Apache Hadoop" href="http://hadoop.apache.org/" target="_blank">Hadoop</a> and <a title="Ubuntu" href="http://www.ubuntu.com/" target="_blank">Ubuntu</a> - step 2 - Configure SSH and user for SSH on Ubuntu
<br/><br/>
In the step 1 of the set up available <a title="Step 1" href="hadoop-ubuntu-step-1.html" target="_blank">here</a>,  step 2 is available <a title="Step 2" href="hadoop-ubuntu-step-2-install-oracle-java.html" target="_blank">here</a>. We took a look at installation of Linux based OS (Ubuntu) for Hadoop as we opted for Linux instead of Windows for Hadoop. We also saw the reasons for the preference. We installed and Configured our chosed Java - Oracle Java.
<br/><br/>
<a title="Step 1" href="hadoop-ubuntu-step-1/" target="_blank">STEP 1</a>- Choose and configure (Linux) OS of choice on Machine of Choice
<br/><br/>
<a title="Step 2" href="hadoop-ubuntu-step-2-install-oracle-java.html" target="_blank">STEP 2</a> - Install Java and configure it on the machine
<br/><br/>
STEP 3 - Configure SSH and user for SSH on Ubuntu
<!-- Collapse text from here -->
<div class="collapse">
<br/><br/>
<a href="00_01.png"><img alt="00" src="00_01.png" width="300" height="202" /></a>
<br/><br/>
This step is pretty straight forward. We create a user and a user group. All hadoop cluster nodes will be using similar user name and will be part of same group. Lets call the group as Hadoop and the user as HdUser. Then we will create a SSH with its RSA key and no authentication (for ease of access by Hadoop).
<br/><br/>
Use Ubuntu Terminal window and below commands.
<br/><br/>
Create group
<br/>
<pre>sudo addgroup hadoop</pre>
<br/>
Create User and add it to the group
<br/>
<pre>sudo adduser --ingroup hadoop hduser</pre>
<br/><br/>
Login as HdUser and generate SSH key
<br/>
<pre>su - hduser</pre>
<br/>
<pre>ssh-keygen -t rsa -P ""</pre>
<br/>
<pre><strong>Generating public/private rsa key pair. Enter file in which to save the key (/home/hduser/.ssh/id_rsa): Created directory '/home/hduser/.ssh'. Your identification has been saved in /home/hduser/.ssh/id_rsa. Your public key has been saved in /home/hduser/.ssh/id_rsa.pub. The key fingerprint is: 7b:62:&lt;&lt;more hex codes&gt;&gt;
<br/>
hduser@ubuntu The key's randomart image is: &lt;&lt;som image&gt;&gt; </strong></pre>
<br/>
Store the generated key
<br/>
<pre>cat $HOME/.ssh/id_rsa.pub &gt;&gt; $HOME/.ssh/authorized_keys</pre>
<br/>
Test SSH
<br/>
<pre>ssh localhost</pre>
<br/>
<pre><strong>The authenticity of host 'localhost (::1)' can't be established. RSA key fingerprint is c7:47:55:&lt;&lt;more hex code&gt;&gt;.
<br/>
Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added 'localhost' (RSA) to the list of known hosts. Linux ubuntu 2.6.32-22-generic #33-Ubuntu SMP Wed Apr 28 13:27:30 UTC 2010 i686 GNU/Linux Ubuntu 10.04 LTS &lt;&lt;info&gt;&gt;</strong></pre>
<br/><br/>
This completes step 3. You can also learn about <a title="uses of Hadoop" href="usage-hadoop.html" target="_blank">usage of Hadoop</a> and about <a title="Hadoop architecture" href="hadoop-architecture-birdview.html" target="_blank">Hadoop architecture </a>on <a title="BabaGyan.com" href="http://babagyan.com/" target="_blank">BabaGyan.com</a>.
<br/><br/>

<!-- Collapse button here -->
</div>
<p><a class="btn btn-success btn-lg" href="#">Click to read more &raquo;</a></p>
</div> </div>
<!-- Collapse group end -->

<!-- Collapse group start -->
<div class="row">
<div class="collapse-group">

	<h3>
	<a title="BabaGyan.com | Blog Post" href="hadoop-ubuntu-step-4.html">Hadoop and Ubuntu - step 4</a>
	</h3>

	<hr id="j1-hr">
<a title="Apache Hadoop" href="http://hadoop.apache.org/" target="_blank">Hadoop</a> and <a title="Ubuntu" href="http://www.ubuntu.com/" target="_blank">Ubuntu</a> - step 4 - Install and configure Hadoop is the last step for creating the single node of hadoop
<br/><br/>
In the step 1 of the set up available <a title="Step 1" href="hadoop-ubuntu-step-1.html" target="_blank">here</a>, step 2 is available <a title="Step 2" href="hadoop-ubuntu-step-2-install-oracle-java.html" target="_blank">here</a>. We took a look at installation of Linux based OS (Ubuntu) for Hadoop as we opted for Linux instead of Windows for Hadoop. We also saw the reasons for the preference. We installed and Configured our chosed Java - Oracle Java.
<br/><br/>
<a title="Step 1" href="hadoop-ubuntu-step-1.html" target="_blank">STEP 1</a> - Choose and configure (Linux) OS of choice on Machine of Choice
<br/><br/>
<a title="Step 2" href="hadoop-ubuntu-step-2-install-oracle-java.html" target="_blank">STEP 2</a> - Install Java and configure it on the machine
<br/><br/>
<a title="Step 3" href="hadoop-ubuntu-step-3-set-up-ssh.html" target="_blank">STEP 3</a> - Configure SSH and user for SSH on Ubuntu
<br/><br/>
STEP 4 - Download and Configure Hadoop
<!-- Collapse text from here -->
<div class="collapse">
<a href="images.jpg"><img class="alignleft size-full wp-image-666" alt="Hadoop" src="images.jpg" width="223" height="226" /></a>
<br/><br/>
<br/><br/>
Login as HDUser. Download Hadoop - 2.x tar file from the any mirror <a title="Hadoop official download mirrors" href="http://www.apache.org/dyn/closer.cgi/hadoop/common/" target="_blank">here</a>
<br/><br/>
Uncompress the Hadoop tar gz file and move it to /usr/local. We will also change owner. Use Terminal for all commands
<br/><br/>

<pre>cd Downloads

sudo tar vxzf hadoop-2.2.0.tar.gz -C /usr/local

cd /usr/local

sudo mv hadoop-2.2.0 hadoop

sudo chown -R hduser:hadoop hadoop</pre>
<br/>
Update the HDUser's .bashrc file
<br/>
<pre>cd ~
<br/>
gksudo gedit .bashrc</pre>
<br/>
<pre>Update the file at the end with below text. Use jdk folder name same as actual folder - something like "jdk-7-i386" (check in /usr/lib/jvm)
<br/>
#Hadoop variables

export JAVA_HOME=/usr/lib/jvm/jdk/

export HADOOP_INSTALL=/usr/local/hadoop

export PATH=$PATH:$HADOOP_INSTALL/bin

export PATH=$PATH:$HADOOP_INSTALL/sbin

export HADOOP_MAPRED_HOME=$HADOOP_INSTALL

export HADOOP_COMMON_HOME=$HADOOP_INSTALL

export HADOOP_HDFS_HOME=$HADOOP_INSTALL

export YARN_HOME=$HADOOP_INSTALL

#end of update</pre>


Save and close
<br/><br/>
Now open hadoop-env.sh for updating Java Home (JAVA_HOME)
<br/>
<pre>gksudo gedit /usr/local/hadoop/etc/hadoop/hadoop-env.sh</pre>
<br/>
<pre>export JAVA_HOME=/usr/lib/jvm/jdk/</pre>
<br/>
Save and close. Reboot the system and login with HDUser again.
<br/><br/>
Now, verify Hadoop installation for terminal
<br/>
<pre>hadoop version</pre>
<br/>
This should give something like below
<br/><br/>
<span style="color: #339966;"><strong>Hadoop 2.2.0</strong></span>
<br/>
<div dir="ltr"><span style="color: #339966;"><strong>Subversion https://svn.apache.org/repos/asf/hadoop/common -r 1529768</strong></span></div>
<br/>
<div dir="ltr"><span style="color: #339966;"><strong>Compiled by hortonmu on 2013-10-07T06:28Z</strong></span></div>
<br/>
<div dir="ltr"><span style="color: #339966;"><strong>Compiled with protoc 2.5.0</strong></span></div>
<br/>
<div dir="ltr"><span style="color: #339966;"><strong>From source with checksum 79e53ce7994d1628b240f09af91e1af4</strong></span></div>
<br/>
<div dir="ltr"><span style="color: #339966;"><strong>This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-2.2.0.jar</strong></span></div>
<br/>
<div dir="ltr"></div>
<br/>
If you get it, Congratulations!!! Hadoop is now successfully installed. If not, put me a comment on contact page
<br/><br/>
Now we configure it by updating its xml files
<br/><br/>
Open core-site.xml and add the given text between <span style="color: #993300;">&lt;configuration&gt; &lt;/configuration&gt;</span> tags
<br/>
<pre>gksudo gedit /usr/local/hadoop/etc/hadoop/core-site.xml</pre>
<br/>
<span style="color: #993300;">&lt;property&gt;</span>
<br/>
<span style="color: #993300;">&lt;name&gt;fs.default.name&lt;/name&gt;</span>
<br/>
<span style="color: #993300;">&lt;value&gt;hdfs://localhost:9000&lt;/value&gt;</span>
<br/>
<span style="color: #993300;">&lt;/property&gt;</span>
<br/><br/>
Save and Close the file
<br/><br/>
Open yarn-site.xml and add the given text between<span style="color: #993300;"> &lt;configuration&gt; &lt;/configuration&gt;</span> tags
<br/>
<pre>gksudo gedit /usr/local/hadoop/etc/hadoop/yarn-site.xml</pre>
<br/>
<span style="color: #993300;">&lt;property&gt;</span>
<br/>
<span style="color: #993300;"> &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span>
<br/>
<span style="color: #993300;"> &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span>
<br/>
<span style="color: #993300;">&lt;/property&gt;</span>
<br/><br/>
<span style="color: #993300;">&lt;property&gt;</span>
<br/>
<span style="color: #993300;"> &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;</span>
<br/>
<span style="color: #993300;"> &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;</span>
<br/>
<span style="color: #993300;">&lt;/property&gt;</span>
<br/><br/>
Save and Close the file
<br/><br/>
Open mapred-site.xml.template and add the given text between <span style="color: #993300;">&lt;configuration&gt; &lt;/configuration&gt;</span> tags
<br/>
<pre>gksudo gedit /usr/local/hadoop/etc/hadoop/mapred-site.xml.template</pre>
<br/>
<span style="color: #993300;">&lt;property&gt;</span>
<br/>
<span style="color: #993300;"> &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span>
<br/>
<span style="color: #993300;"> &lt;value&gt;yarn&lt;/value&gt;</span>
<br/>
<span style="color: #993300;">&lt;/property&gt;</span>
<br/><br/>
Save the file <span style="color: #ff0000;">as mapred-site.xml in /usr/local/hadoop/etc/hadoop/</span> directory and Close the file
<br/><br/>
Lets now create Name Node and Data Node through terminal
<br/>
<pre>cd ~

mkdir -p mydata/hdfs/namenode

mkdir -p mydata/hdfs/datanode</pre>

Now, update hdfs-site.xml and add the given text between <span style="color: #993300;">&lt;configuration&gt; &lt;/configuration&gt;</span> tags

<pre>gksudo gedit /usr/local/hadoop/etc/hadoop/hdfs-site.xml</pre>
<br/>
<span style="color: #993300;">&lt;property&gt;</span>
<br/>
<span style="color: #993300;"> &lt;name&gt;dfs.replication&lt;/name&gt;</span>
<br/>
<span style="color: #993300;"> &lt;value&gt;1&lt;/value&gt;</span>
<br/>
<span style="color: #993300;">&lt;/property&gt;</span>
<br/><br/>
<span style="color: #993300;">&lt;property&gt;</span>
<br/>
<span style="color: #993300;"> &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span>
<br/>
<span style="color: #993300;"> &lt;value&gt;file:/home/hduser/mydata/hdfs/namenode&lt;/value&gt;</span>
<br/>
<span style="color: #993300;">&lt;/property&gt;</span>
<br/><br/>
<span style="color: #993300;">&lt;property&gt;</span>
<br/>
<span style="color: #993300;"> &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span>
<br/>
<span style="color: #993300;"> &lt;value&gt;file:/home/hduser/mydata/hdfs/datanode&lt;/value&gt;</span>
<br/>
<span style="color: #993300;">&lt;/property&gt;</span>
<br/><br/>
Next, we will format the hdfs for our first use of Hadoop and start the Hadoop Services
<br/>
<pre>hdfs namenode -format</pre>
<br/>
<pre>start-dfs.sh</pre>
<br/>
<pre>start-yarn.sh</pre>
<br/>
Verify the Hadoop nodes running by
<br/>
<pre>jps</pre>
<br/>
The below should appear in output
<br/><br/>

<span style="color: #339966;">2970 ResourceManager
<br/>
3461 Jps
<br/>
3177 NodeManager
<br/>
2361 NameNode
<br/>
2840 SecondaryNameNode
<br/>
<br/>
</span>
This completes the set up steps for Hadoop.
<br/><br/>
You can also learn about <a title="uses of Hadoop" href="usage-hadoop.html" target="_blank">usage of Hadoop</a> and about <a title="Hadoop architecture" href="hadoop-architecture-birdview.html" target="_blank">Hadoop architecture</a> on <a title="BabaGyan.com" href="http://babagyan.com/" target="_blank">BabaGyan.com</a>.

<!-- Collapse button here -->
</div>
<p><a class="btn btn-success btn-lg" href="#">Click to read more &raquo;</a></p>
</div> </div>
<!-- Collapse group end -->

<!-- Collapse group start -->
<div class="row">
<div class="collapse-group">

	<h3>
	<a title="BabaGyan.com | Blog Post" href="solutions-issues-faced-hadoop-configuration.html">Solutions for issues faced during hadoop configuration</a>
	</h3>
	<hr id="j1-hr">
Solutions for issues faced during hadoop configuration - starting as begineer for Hadoop use is not so straight forward. There are many steps and issues that one has to overcome. I have done it and so putting this up for everyone to refer. Step by step guide for Hadoop configuration is also available here - <a title="Step 1" href="hadoop-ubuntu-step-1.html" target="_blank">STEP 1</a>, <a title="Step 2" href="hadoop-ubuntu-step-2-install-oracle-java.html" target="_blank">STEP 2</a>, <a title="Step 3" href="hadoop-ubuntu-step-3-set-up-ssh.html" target="_blank">STEP 3</a> and <a title="Step 4" href="hadoop-ubuntu-step-4.html" target="_blank">STEP 4</a>.
<br/><br/>
Solutions for issues faced during hadoop configuration
<br/><br/>
1. Which Hadoop to fetch:
<!-- Collapse text from here -->
<div class="collapse">
<a href="images.jpg"><img class="alignleft size-full wp-image-666" alt="Hadoop" src="images.jpg" width="223" height="226" /></a>
<br/><br/>
<br/>
There are two flavors of hadoop - 1.x and 2.x.
<br/>
The 1.x is the initial one while 2.x was a parallel version which had YARN engine in it. So, go for Hadoop 2.x version
<br/>
You can find more about hadoop <a title="Hadoop downloads" href="https://hadoop.apache.org/releases.html#Download" target="_blank">here</a>
<br/><br/>
2. Which machine to use:
<br/>
Initial options are Windows and Linux. Since SSH will be extensively used, prefer a flavor of Linux for Hadoop. It will also eliminate the need to licence each instance/node that you will create.
<br/>
Prefer Ubuntu if you are a extensive Windows user since you will not feel completely lost in the Unix like environment. Also, there is lot of online help on Ubuntu.
<br/>
Use <a title="Ubuntu on Virtual Box" href="hadoop-ubuntu-step-1.html" target="_blank">this guide</a> for downloading Ubuntu and installing it on VM
<br/><br/>
3. Actual machines or Virtual machines:
<br/>
I guess this is pretty easy to decide. Virtual machines offcorse. Will need atleast one actual machine with latest configurations and atleast 4GB RAM for VMs to run.
<br/><br/>
4. Which Virtualization environment:
<br/>
There are many options but most popular will be Virtual Box by Oracle and VMWare. Virtual box is free and open source. Support wise it is good enough online so prefer Virtual Box.
<br/>
You can find how to set up the box <a title="Ubuntu on Virtual Box" href="hadoop-ubuntu-step-1.html" target="_blank">here</a>
<br/><br/>
4. Which Java to use:
<br/>
Most common Java versions for Linux based systems are OpenJDK; and there is always Oracle JDK available. As per the hadoop docomentation, choose a java version. It is best to go for Oracle JDK but an older and test version of Java.
<br/><br/>
You can find how to install java <a title="Java on Ubuntu" href="hadoop-ubuntu-step-2-install-oracle-java.html" target="_blank">here</a>
<br/><br/>
Major Issues:
<br/><br/>
1. Java and Ubuntu - 32 bit or 64 bit
<br/>
If your machine is latest one as 64 bit, you may be tempted to go fir a 64 bit version of OS as well as Java. But just don't go for it yet.
<br/><br/>
Hadoop native libraries are compiled for 32 bit and if you are using 64 bit OS, you may run into problems and errors such as:
<br/><br/>
<span style="color: #ff0000;">WARN util.NativeCodeLoader: Unable to load native-hadoop library for </span>
<br/>
<span style="color: #ff0000;">your platform...</span>
<br/>
<span style="color: #ff0000;">using builtin-java classes where applicable</span>
<br/><br/>
The solution to this is recompiling the Hadoop native libraries in your 64 bit machine. Have a look at the <a title="Hadoop Native Libraries" href="http://hadoop.apache.org/docs/r2.2.0/hadoop-project-dist/hadoop-common/NativeLibraries.html" target="_blank">native libraries page</a> and <a title="Building Hadoop native libraries" href="https://svn.apache.org/repos/asf/hadoop/common/trunk/BUILDING.txt" target="_blank">building native libraries for hadoop</a> page.

But then, it will be lot better to use 32 bit OS instead, isn't it?
<br/>
(shout yes!!!)
<br/><br/>
2. Virtual Box - low graphics mode
<br/>
Virtual box may run into error as Low graphics is On if you are using 32 bit Ubuntu. This is due to a missing guest plugin which comes with Virtual box.
<br/>
You will have to run the Linux Guest CD image and load it. For this, the initial step is setting the Ubuntu to run kernel commands
<br/>
<pre>sudo apt-get install dkms</pre>
then load the Guest addon CD
<pre>sudo mount /dev/cdrom /cdrom
sudo sh ./VBoxLinuxAdditions.run</pre>
<br/><br/>
3. Virtual Box - mouse pointer appears little above the point
<br/>
This is due to a missing patch. You can have a look at it <a title="Virtual box - mouse pointer mismatch patch" href="https://www.virtualbox.org/ticket/2306" target="_blank">here</a>.
<br/><br/>
For fixing this, download the <span style="color: #993300;">VBoxGuest-linux.c.patch</span> patch file from above link. Then run these commands on your Ubuntu virtual machine
<pre>cp VBoxGuest-linux.c.patch /usr/src/vboxguest-4.1.16/vboxguest/VBoxGuest-linux.c</pre>
<pre>/etc/init.d/vboxadd setup</pre>
You can also learn about <a title="uses of Hadoop" href="usage-hadoop.html" target="_blank">usage of Hadoop</a> and about <a title="Hadoop architecture" href="hadoop-architecture-birdview.html" target="_blank">Hadoop architecture </a>on <a title="BabaGyan.com" href="http://babagyan.com/" target="_blank">BabaGyan.com</a>.
<br/><br/>


<!-- Collapse button here -->
</div>
<p><a class="btn btn-success btn-lg" href="#">Click to read more &raquo;</a></p>
</div> </div>
<!-- Collapse group end -->

<!-- Tag Cloud start -->

<div class="container col-md-12">
<ul class="nav nav-pills pull-right" id="tagcloud">
<h3> <hr id="j1-hr"/> </h3>
</ul>
</div>

<div class="container col-md-12" id="tagcloud">
<ul class="nav nav-pills pull-center" id="tags">
<a href="tag-hadoop.html" class="tag1">Hadoop</a>
<a href="tag-msbi.html" class="tag2">MSBI</a>
<a href="tag-etl-bi.html" class="tag3">ETL & BI</a>
<a href="tag-bigdata.html" class="tag4">Bigdata</a>
<a href="tag-reviews.html" class="tag5">Reviews</a>
<a href="tag-case-study.html" class="tag6">Case Study</a>
<a href="tag-others.html" class="tag10">Others</a>
</ul>
</div>
<!-- Tag Cloud end -->

</div>

<!-- Jumbotron 1 end -->


</div>

<!-- Container 1 end -->

<div class="jumbotron col-md-4" id="j2">
	<h3>Also Read
	</h3>
<hr id="j2-hr"/>
<ul class="list-inline" id="j2-side"> <li> <a title="BabaGyan.com | Blog Post" href="case-study-multiply-revenue-and-customer-base.html">Case Study: Multiply Revenue as well as Customer Base</a></li><li> <a title="BabaGyan.com | Blog Post" href="tool-project-tracker.html">Tool: Project Stages Tracker</a></li><li> <a title="BabaGyan.com | Blog Post" href="exploring-cloud-on-platform-google.html">Exploring Cloud platform on Google</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="babagyan-com-exploring-ghost-blogging-platform-on-bitnami.html">BabaGyan.com Exploring Ghost Blogging platform on Bitnami</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="solutions-issues-faced-hadoop-configuration.html">Solutions for issues faced during hadoop configuration</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="hadoop-ubuntu-step-4.html">Hadoop and Ubuntu - step 4</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="hadoop-ubuntu-step-3-set-up-ssh.html">Hadoop and Ubuntu - step 3</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="hadoop-ubuntu-step-2-install-oracle-java.html">Hadoop and Ubuntu - step 2</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="hadoop-ubuntu-step-1.html">Hadoop and Ubuntu - step 1</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="usage-hadoop.html">Usage of Hadoop</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="hadoop-architecture-birdview.html">Hadoop architecture birdview</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="pros-cons-gmail-displaying-images-emails.html">Pros and Cons of Gmail displaying images in your emails</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="clt20-2013-twitter-analysis.html">CLT20 2013 Twitter Analysis</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="etl-bi-bigdata-past-and-future.html">ETL BI BigData Past Future</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="tool-daily-expense-tracker.html">Tool: Daily Expense Tracker</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="ways-create-website.html">Lifesaver: Ways to create a website</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="lifesaver-business-intelligence-projects.html">LifeSaver: Business Intelligence Projects - Success and Failures</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="lifesaver-differences-between-udt-and-idt.html">LifeSaver: Differences between UDT and IDT</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="data-quality-in-etl-and-bi.html">Data Quality in ETL and BI</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="problem-solving-methodology.html">Problem Solving Methodology</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="what-is-mentoring.html">What is Mentoring</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="qlikview-advantage.html">QlikView advantage over query based BI</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="types-data.html">Types of data</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="lyrics-one-two-three-four-chennai-express.html">Lyrics One Two Three Four Chennai Express</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="qlikview-licensing-an-overview.html">QlikView Licensing - an Overview</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="baba-jee-ki-booti.html">Theme song lyrics - Baba jee ki booti</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="tv-oled-led-lcd-plasma.html">TV - OLED vs LED vs LCD vs Plasma</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="review-google-moto-x.html">Review Google Moto X</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="blackberry-q10-q5-review.html">Review: Blackberry Q10 and Blackberry Q5</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="lifesaver-ssrs-all-in-one.html">Lifesaver - SSRS All in one</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="lifesaver-ssis-all-in-one.html">Lifesaver - SSIS All in one</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="lifesaver-ssas-mdx-quick-fixes.html">Lifesaver - SSAS MDX Quick fixes</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="lifesaver-ssrs-quick-fixes.html">Lifesaver - SSRS Quick fixes</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="ssis-variables-script-profiling.html">SSIS Package Variables, Script and Profiling</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="lifesaver-ssis-quick-fixes.html">Lifesaver - SSIS Quick fixes</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="ssis-control-flow-containers.html">SSIS Control Flow Containers</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="ssis-control-flow-objects.html">SSIS Control Flow Objects</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="ssis-exercise-05.html">SSIS Exercise 05</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="ssis-exercise-04.html">SSIS Exercise 04</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="ssis-exercise-03.html">SSIS Exercise 03</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="ssis-exercise-02.html">SSIS Exercise 02</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="ssis-exercise-01.html">SSIS Exercise 01</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="theme-song-lyrics-the-big-bang-theory.html">Theme song lyrics - The Big Bang Theory</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="theme-song-lyrics-friends.html">Theme song lyrics - F.R.I.E.N.D.S</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="sql-aggregate-analytic-functions.html">SQL - Aggregate & Analytic functions</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="list-of-analytical-functions.html">SQL - List of Analytical functions</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="sql-statement.html">SQL - Where, Operators and alias</a>
</li><li> <a title="BabaGyan.com | Blog Post" href="sql-structured-query-language-for-database-quering.html">SQL - Structured Query Language for Database quering</a>
</li>
</ul>
</div>

    </div> <!-- /container -->


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster 	-->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="../js/bootstrap.min.js"></script>
    <script src="../js/babagyan.js"></script>
   <script src="../js/babagyan.js"></script>  <script> trackReferrer(); </script>  <footer class="bg-secondary">    <div class="text-center text-white pb-3">      Our Other Creations </br>      |      <a href=https://ChilliClove.com class="text-white">ChilliClove.com|</a>        <a href=https://MyLoveMsg.in class="text-white">MyLoveMsg.in|</a>    </br></br>        <p class="mb-0">Copyright ©<script>var CurrentYear = new Date().getFullYear()      document.write(CurrentYear)</script>          <a href="/host/dhanesh/" class="text-white">Dhanesh Aradhye</a>.                          All Rights Reserved. </p>        |             <a href="/privacy-policy" class="text-white">Privacy policy |</a>            <a href="/terms-of-use" class="text-white">Terms of Use |</a>            </div>  </footer>      </body>
</html>
